{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristValen/ML-Python/blob/main/python_svm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vhTfpo5mmUZ"
      },
      "outputs": [],
      "source": [
        "#svm\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Establecer la semilla para el generador de números aleatorios\n",
        "random_state = 42\n",
        "\n",
        "# Convertir el DataFrame de PySpark a Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Dividir el conjunto de datos en train y test\n",
        "X = pandas_df.drop('Malos_Dias_tot', axis=1)\n",
        "y = pandas_df['Malos_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Estandarizar las variables\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Entrenar el modelo SVM con validación cruzada\n",
        "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
        "svc = svm.SVC(probability=True, random_state=random_state)\n",
        "clf = GridSearchCV(svc, parameters)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predecir valores para el conjunto de test\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "# Calcular métricas\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calcular el estadístico KS\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test_scaled)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Obtener las 10 variables más importantes del modelo\n",
        "feature_importances = pd.DataFrame(clf.best_estimator_.coef_[0], index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Establecer la semilla para el generador de números aleatorios\n",
        "random_state = 42\n",
        "\n",
        "# Convertir el DataFrame de PySpark a Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Dividir el conjunto de datos en train y test\n",
        "X = pandas_df.drop('Malos_Dias_tot', axis=1)\n",
        "y = pandas_df['Malos_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Estandarizar las variables\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento\n",
        "sm = SMOTE(random_state=random_state)\n",
        "X_train_resampled, y_train_resampled = sm.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Entrenar el modelo SVM\n",
        "clf = svm.SVC(kernel='linear', probability=True, random_state=random_state)\n",
        "clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predecir valores para el conjunto de test\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "# Calcular métricas\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calcular el estadístico KS\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test_scaled)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Obtener las 10 variables más importantes del modelo\n",
        "feature_importances = pd.DataFrame(clf.coef_[0], index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n",
        "\n",
        "#\n"
      ],
      "metadata": {
        "id": "p72DoTO3mpeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#undersampling\n",
        "# Establecer la semilla para el generador de números aleatorios\n",
        "random_state = 42\n",
        "\n",
        "# Convertir el DataFrame de PySpark a Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Dividir el conjunto de datos en train y test\n",
        "X = pandas_df.drop('Malos_Dias_tot', axis=1)\n",
        "y = pandas_df['Malos_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Aplicar RandomUnderSampler al conjunto de entrenamiento\n",
        "rus = RandomUnderSampler(random_state=random_state)\n",
        "X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "# Crear una instancia de StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Ajustar el escalador a los datos de entrenamiento\n",
        "scaler.fit(X_train_resampled)\n",
        "\n",
        "# Transformar los datos de entrenamiento y prueba\n",
        "X_train_resampled = scaler.transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Entrenar el modelo SVM con validación cruzada\n",
        "param_grid = {'C': [0.1, 1, 10], 'gamma': [1, 0.1, 0.01]}\n",
        "grid = GridSearchCV(svm.SVC(kernel='linear', probability=True, random_state=random_state), param_grid, refit=True)\n",
        "grid.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predecir valores para el conjunto de test\n",
        "y_pred = grid.predict(X_test)\n",
        "\n",
        "# Calcular métricas\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calcular el estadístico KS\n",
        "fpr, tpr, thresholds = roc_curve(y_test, grid.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Obtener las 10 variables más importantes del modelo\n",
        "feature_importances = pd.DataFrame(grid.best_estimator_.coef_[0], index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sQmvjPzmnIJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Establecer la semilla para el generador de números aleatorios\n",
        "random_state = 42\n",
        "\n",
        "# Convertir el DataFrame de PySpark a Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Dividir el conjunto de datos en train y test\n",
        "X = pandas_df.drop('Malos_Dias_tot', axis=1)\n",
        "y = pandas_df['Malos_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Aplicar ADASYN al conjunto de entrenamiento\n",
        "adasyn = ADASYN(random_state=random_state)\n",
        "X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)\n",
        "\n",
        "# Crear una instancia de StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Ajustar el escalador a los datos de entrenamiento\n",
        "scaler.fit(X_train_resampled)\n",
        "\n",
        "# Transformar los datos de entrenamiento y prueba\n",
        "X_train_resampled = scaler.transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Entrenar el modelo SVM con validación cruzada\n",
        "param_grid = {'C': [0.1, 1, 10], 'gamma': [1, 0.1, 0.01]}\n",
        "grid = GridSearchCV(svm.SVC(kernel='linear', probability=True, random_state=random_state), param_grid, refit=True)\n",
        "grid.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predecir valores para el conjunto de test\n",
        "y_pred = grid.predict(X_test)\n",
        "\n",
        "# Calcular métricas\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calcular el estadístico KS\n",
        "fpr, tpr, thresholds = roc_curve(y_test, grid.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Obtener las 10 variables más importantes del modelo\n",
        "feature_importances = pd.DataFrame(grid.best_estimator_.coef_[0], index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jVm7_ta7nYyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Establecer la semilla para el generador de números aleatorios\n",
        "random_state = 42\n",
        "\n",
        "# Convertir el DataFrame de PySpark a Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Dividir el conjunto de datos en train y test\n",
        "X = pandas_df.drop('Malos_Dias_tot', axis=1)\n",
        "y = pandas_df['Malos_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Aplicar TomekLinks al conjunto de entrenamiento\n",
        "tl = TomekLinks()\n",
        "X_train_resampled, y_train_resampled = tl.fit_resample(X_train, y_train)\n",
        "\n",
        "# Crear una instancia de StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Ajustar el escalador a los datos de entrenamiento\n",
        "scaler.fit(X_train_resampled)\n",
        "\n",
        "# Transformar los datos de entrenamiento y prueba\n",
        "X_train_resampled = scaler.transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Definir los parámetros para la búsqueda en grilla\n",
        "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
        "\n",
        "# Crear el objeto GridSearchCV\n",
        "grid_search = GridSearchCV(svm.SVC(probability=True, random_state=random_state), param_grid, cv=5)\n",
        "\n",
        "# Entrenar el modelo SVM con búsqueda en grilla\n",
        "grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predecir valores para el conjunto de test\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "# Calcular métricas\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calcular el estadístico KS\n",
        "fpr, tpr, thresholds = roc_curve(y_test, grid_search.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Obtener las 10 variables más importantes del modelo\n",
        "feature_importances = pd.DataFrame(grid_search.best_estimator_.coef_[0], index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n",
        "\n"
      ],
      "metadata": {
        "id": "3MlMop7Rnn-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sin cross validation\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 42\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X = pandas_df.drop('Malos_Dias_tot', axis=1)\n",
        "y = pandas_df['Malos_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Standardize the variables\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the SVM model without cross-validation\n",
        "svc = svm.SVC(probability=True, random_state=random_state)\n",
        "svc.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict values for the test set\n",
        "y_pred = svc.predict(X_test_scaled)\n",
        "\n",
        "# Calculate metrics\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate the KS statistic\n",
        "fpr, tpr, thresholds = roc_curve(y_test, svc.predict_proba(X_test_scaled)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "feature_importances = pd.DataFrame(svc.coef_[0], index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kLC_hmQw6_E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 42\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X = pandas_df.drop('Malos_Dias_tot', axis=1)\n",
        "y = pandas_df['Malos_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Standardize the variables\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Apply SMOTE to the training set\n",
        "sm = SMOTE(random_state=random_state)\n",
        "X_train_resampled, y_train_resampled = sm.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Train the SVM model without cross-validation\n",
        "clf = svm.SVC(kernel='linear', probability=True, random_state=random_state)\n",
        "clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict values for the test set\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "# Calculate metrics\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate the KS statistic\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test_scaled)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "feature_importances = pd.DataFrame(clf.coef_[0], index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n"
      ],
      "metadata": {
        "id": "fK3m-f5D73Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 42\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X = pandas_df.drop('Malos_Dias_tot', axis=1)\n",
        "y = pandas_df['Malos_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Apply RandomUnderSampler to the training set\n",
        "rus = RandomUnderSampler(random_state=random_state)\n",
        "X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data\n",
        "scaler.fit(X_train_resampled)\n",
        "\n",
        "# Transform the training and test data\n",
        "X_train_resampled = scaler.transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the SVM model without cross-validation\n",
        "clf = svm.SVC(kernel='linear', probability=True, random_state=random_state)\n",
        "clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict values for the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate the KS statistic\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "feature_importances = pd.DataFrame(clf.coef_[0], index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n"
      ],
      "metadata": {
        "id": "GzRu8wmc8ZAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import ADASYN\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 42\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X = pandas_df.drop('Malos_Dias_tot', axis=1)\n",
        "y = pandas_df['Malos_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Apply ADASYN to the training set\n",
        "adasyn = ADASYN(random_state=random_state)\n",
        "X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data\n",
        "scaler.fit(X_train_resampled)\n",
        "\n",
        "# Transform the training and test data\n",
        "X_train_resampled = scaler.transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the SVM model without cross-validation\n",
        "clf = svm.SVC(kernel='linear', probability=True, random_state=random_state)\n",
        "clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict values for the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate the KS statistic\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "feature_importances = pd.DataFrame(clf.coef_[0], index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n"
      ],
      "metadata": {
        "id": "55hF-yWA8ZH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 42\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X = pandas_df.drop('Malos_Dias_tot', axis=1)\n",
        "y = pandas_df['Malos_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Apply TomekLinks to the training set\n",
        "tl = TomekLinks()\n",
        "X_train_resampled, y_train_resampled = tl.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data\n",
        "scaler.fit(X_train_resampled)\n",
        "\n",
        "# Transform the training and test data\n",
        "X_train_resampled = scaler.transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the SVM model without cross-validation\n",
        "clf = svm.SVC(kernel='linear', probability=True, random_state=random_state)\n",
        "clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict values for the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate the KS statistic\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "feature_importances = pd.DataFrame(clf.coef_[0], index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n"
      ],
      "metadata": {
        "id": "je6qKRmA8ZKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gasHXNhX8ZN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UghiLkLQ8ZSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#random forest\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 0\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X = pandas_df.drop('Malo_Dias_tot', axis=1)\n",
        "y = pandas_df['Malo_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Transform the training and test data\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the Random Forest model without cross-validation\n",
        "clf = RandomForestClassifier(random_state=random_state)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict values for the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate the KS statistic\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "feature_importances = pd.DataFrame(clf.feature_importances_, index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n",
        "\n"
      ],
      "metadata": {
        "id": "hr0MIUTB6rDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 0\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X = pandas_df.drop('Malo_Dias_tot', axis=1)\n",
        "y = pandas_df['Malo_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Apply SMOTE to the training set\n",
        "sm = SMOTE(random_state=random_state)\n",
        "X_train_resampled, y_train_resampled = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data\n",
        "scaler.fit(X_train_resampled)\n",
        "\n",
        "# Transform the training and test data\n",
        "X_train_resampled = scaler.transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the Random Forest model without cross-validation\n",
        "clf = RandomForestClassifier(random_state=random_state)\n",
        "clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict values for the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate the KS statistic\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "feature_importances = pd.DataFrame(clf.feature_importances_, index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n"
      ],
      "metadata": {
        "id": "iorfuzKbDm8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 0\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X = pandas_df.drop('Malo_Dias_tot', axis=1)\n",
        "y = pandas_df['Malo_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Apply RandomUnderSampler to the training set\n",
        "rus = RandomUnderSampler(random_state=random_state)\n",
        "X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data\n",
        "scaler.fit(X_train_resampled)\n",
        "\n",
        "# Transform the training and test data\n",
        "X_train_resampled = scaler.transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the Random Forest model without cross-validation\n",
        "clf = RandomForestClassifier(random_state=random_state)\n",
        "clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict values for the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate the KS statistic\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "feature_importances = pd.DataFrame(clf.feature_importances_, index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n"
      ],
      "metadata": {
        "id": "pmx9xDOQDnPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import ADASYN\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 0\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X = pandas_df.drop('Malo_Dias_tot', axis=1)\n",
        "y = pandas_df['Malo_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Apply ADASYN to the training set\n",
        "adasyn = ADASYN(random_state=random_state)\n",
        "X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data\n",
        "scaler.fit(X_train_resampled)\n",
        "\n",
        "# Transform the training and test data\n",
        "X_train_resampled = scaler.transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the Random Forest model without cross-validation\n",
        "clf = RandomForestClassifier(random_state=random_state)\n",
        "clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict values for the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate the KS statistic\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "feature_importances = pd.DataFrame(clf.feature_importances_, index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n"
      ],
      "metadata": {
        "id": "vlz_5d1wDnVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### decision tree\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 0\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X = pandas_df.drop('Malo_Dias_tot', axis=1)\n",
        "y = pandas_df['Malo_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Transform the training and test data\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the Decision Tree model without cross-validation\n",
        "clf = DecisionTreeClassifier(random_state=random_state)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict values for the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate the KS statistic\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "feature_importances = pd.DataFrame(clf.feature_importances_, index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n"
      ],
      "metadata": {
        "id": "FNjA5XJKDnYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###pyspark svm\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 0\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train, test = df_2.randomSplit([0.8, 0.2], seed=random_state)\n",
        "\n",
        "# Define the feature columns\n",
        "feature_cols = [col for col in train.columns if col != 'Malo_Dias_tot']\n",
        "\n",
        "# Create a VectorAssembler to combine the feature columns into a single vector column\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "\n",
        "# Create a StandardScaler to standardize the features\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=True)\n",
        "\n",
        "# Train the Support Vector Machine model without cross-validation\n",
        "svm = LinearSVC(featuresCol='scaledFeatures', labelCol='Malo_Dias_tot', maxIter=10, regParam=0.1)\n",
        "\n",
        "# Create a pipeline to chain the assembler, scaler and SVM together\n",
        "pipeline = Pipeline(stages=[assembler, scaler, svm])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model = pipeline.fit(train)\n",
        "\n",
        "# Predict values for the test set\n",
        "predictions = model.transform(test)\n",
        "\n",
        "# Calculate metrics\n",
        "tp = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 1)].count()\n",
        "tn = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 0)].count()\n",
        "fp = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 1)].count()\n",
        "fn = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 0)].count()\n",
        "\n",
        "print(f'Confusion Matrix:\\n[[{tn} {fp}]\\n [{fn} {tp}]]')\n",
        "\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1_score = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1_score}')\n",
        "\n",
        "# Calculate AUC using BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='Malo_Dias_tot', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f'AUC: {auc}')\n",
        "\n",
        "# Calculate KS statistic using BinaryClassificationEvaluator with areaUnderPR metric\n",
        "evaluator.setMetricName('areaUnderPR')\n",
        "ks = evaluator.evaluate(predictions)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "importances = model.stages[-1].coefficients.toArray()\n",
        "importance_df = pd.DataFrame(list(zip(feature_cols, importances)), columns=['Feature', 'Importance']).sort_values('Importance', ascending=False)\n",
        "print(importance_df.head(10))\n",
        "\n",
        "# Calculate and display ROC curve\n",
        "pdf = predictions.select('Malo_Dias_tot', 'rawPrediction').toPandas()\n",
        "fpr, tpr, thresholds = roc_curve(pdf['Malo_Dias_tot'], pdf['rawPrediction'].apply(lambda x: x[1]))\n",
        "plt.plot(fpr, tpr)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KPaUVr82JJez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 0\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train, test = df_2.randomSplit([0.8, 0.2], seed=random_state)\n",
        "\n",
        "# Define the feature columns\n",
        "feature_cols = [col for col in train.columns if col != 'Malo_Dias_tot']\n",
        "\n",
        "# Perform random undersampling to balance the classes in the training data\n",
        "majority_class = train.groupby('Malo_Dias_tot').count().orderBy('count', ascending=False).first()[0]\n",
        "minority_class_count = train.filter(train.Malo_Dias_tot != majority_class).count()\n",
        "undersampled_train = train.filter(train.Malo_Dias_tot == majority_class).sample(False, float(minority_class_count) / train.filter(train.Malo_Dias_tot == majority_class).count(), seed=random_state)\n",
        "undersampled_train = undersampled_train.union(train.filter(train.Malo_Dias_tot != majority_class))\n",
        "\n",
        "# Create a VectorAssembler to combine the feature columns into a single vector column\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "\n",
        "# Create a StandardScaler to standardize the features\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=True)\n",
        "\n",
        "# Train the Support Vector Machine model without cross-validation\n",
        "svm = LinearSVC(featuresCol='scaledFeatures', labelCol='Malo_Dias_tot', maxIter=10, regParam=0.1)\n",
        "\n",
        "# Create a pipeline to chain the assembler, scaler and SVM together\n",
        "pipeline = Pipeline(stages=[assembler, scaler, svm])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model = pipeline.fit(undersampled_train)\n",
        "\n",
        "# Predict values for the test set\n",
        "predictions = model.transform(test)\n",
        "\n",
        "# Calculate metrics\n",
        "tp = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 1)].count()\n",
        "tn = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 0)].count()\n",
        "fp = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 1)].count()\n",
        "fn = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 0)].count()\n",
        "\n",
        "print(f'Confusion Matrix:\\n[[{tn} {fp}]\\n [{fn} {tp}]]')\n",
        "\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1_score = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1_score}')\n",
        "\n",
        "# Calculate AUC using BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='Malo_Dias_tot', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f'AUC: {auc}')\n",
        "\n",
        "# Calculate KS statistic using BinaryClassificationEvaluator with areaUnderPR metric\n",
        "evaluator.setMetricName('areaUnderPR')\n",
        "ks = evaluator.evaluate(predictions)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "importances = model.stages[-1].coefficients.toArray()\n",
        "importance_df = pd.DataFrame(list(zip(feature_cols, importances)), columns=['Feature', 'Importance']).sort_values('Importance', ascending=False)\n",
        "print(importance_df.head(10))\n",
        "\n",
        "# Calculate and display ROC curve\n",
        "pdf = predictions.select('Malo_Dias_tot', 'rawPrediction').toPandas()\n",
        "fpr, tpr, thresholds = roc_curve(pdf['Malo_Dias_tot'], pdf['rawPrediction'].apply(lambda x: x[1]))\n",
        "plt.plot(fpr, tpr)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Q_ASF38lv79I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "from imblearn.over_sampling import ADASYN\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 0\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train, test = df_2.randomSplit([0.8, 0.2], seed=random_state)\n",
        "\n",
        "# Define the feature columns\n",
        "feature_cols = [col for col in train.columns if col != 'Malo_Dias_tot']\n",
        "\n",
        "# Perform ADASYN oversampling to balance the classes in the training data\n",
        "adasyn = ADASYN(random_state=random_state)\n",
        "train_pd = train.toPandas()\n",
        "X_resampled, y_resampled = adasyn.fit_resample(train_pd[feature_cols], train_pd['Malo_Dias_tot'])\n",
        "oversampled_train = spark.createDataFrame(pd.concat([X_resampled, y_resampled], axis=1))\n",
        "\n",
        "# Create a VectorAssembler to combine the feature columns into a single vector column\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "\n",
        "# Create a StandardScaler to standardize the features\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=True)\n",
        "\n",
        "# Train the Support Vector Machine model without cross-validation\n",
        "svm = LinearSVC(featuresCol='scaledFeatures', labelCol='Malo_Dias_tot', maxIter=10, regParam=0.1)\n",
        "\n",
        "# Create a pipeline to chain the assembler, scaler and SVM together\n",
        "pipeline = Pipeline(stages=[assembler, scaler, svm])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model = pipeline.fit(oversampled_train)\n",
        "\n",
        "# Predict values for the test set\n",
        "predictions = model.transform(test)\n",
        "\n",
        "# Calculate metrics\n",
        "tp = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 1)].count()\n",
        "tn = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 0)].count()\n",
        "fp = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 1)].count()\n",
        "fn = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 0)].count()\n",
        "\n",
        "print(f'Confusion Matrix:\\n[[{tn} {fp}]\\n [{fn} {tp}]]')\n",
        "\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1_score = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1_score}')\n",
        "\n",
        "# Calculate AUC using BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='Malo_Dias_tot', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f'AUC: {auc}')\n",
        "\n",
        "# Calculate KS statistic using BinaryClassificationEvaluator with areaUnderPR metric\n",
        "evaluator.setMetricName('areaUnderPR')\n",
        "ks = evaluator.evaluate(predictions)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "importances = model.stages[-1].coefficients.toArray()\n",
        "importance_df = pd.DataFrame(list(zip(feature_cols, importances)), columns=['Feature', 'Importance']).sort_values('Importance', ascending=False)\n",
        "print(importance_df.head(10))\n",
        "\n",
        "# Calculate and display ROC curve\n",
        "pdf = predictions.select('Malo_Dias_tot', 'rawPrediction').toPandas()\n",
        "fpr, tpr, thresholds = roc_curve(pdf['Malo_Dias_tot'], pdf['rawPrediction'].apply(lambda x: x[1]))\n",
        "plt.plot(fpr, tpr)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XC8aSRh43lis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "\n",
        "# Set the dependent variable\n",
        "label = \"label\"\n",
        "\n",
        "# Rename the Malos_Dias_tot column to label and cast it to a double type\n",
        "df8 = df_2.withColumnRenamed(\"Malos_Dias_tot\", label)\n",
        "df8 = df8.withColumn(label, col(label).cast(DoubleType()))\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train, test = df8.randomSplit([0.7, 0.3], seed=0)\n",
        "\n",
        "# Set the independent variables\n",
        "features = train.columns\n",
        "features.remove(label)\n",
        "\n",
        "# Convert the training data to a pandas DataFrame\n",
        "train_pd = train.toPandas()\n",
        "\n",
        "# Create the SMOTE object\n",
        "smote = SMOTE()\n",
        "\n",
        "# Apply SMOTE to the training data\n",
        "X_resampled, y_resampled = smote.fit_resample(train_pd[features], train_pd[label])\n",
        "\n",
        "# Convert the resampled data to a pandas DataFrame\n",
        "train_resampled_pd = pd.DataFrame(X_resampled, columns=features)\n",
        "train_resampled_pd[label] = y_resampled\n",
        "\n",
        "# Convert the resampled data back to a PySpark DataFrame\n",
        "train_resampled = spark.createDataFrame(train_resampled_pd)\n",
        "\n",
        "# Assemble the feature vector\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "\n",
        "# Create the GBTClassifier model\n",
        "gbt = GBTClassifier(labelCol=label, featuresCol=\"features\")\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[assembler, gbt])\n",
        "\n",
        "# Create the parameter grid for cross-validation\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(gbt.maxDepth, [2, 4, 6]) \\\n",
        "    .addGrid(gbt.maxBins, [20, 60]) \\\n",
        "    .addGrid(gbt.maxIter, [10, 20]) \\\n",
        "    .build()\n",
        "\n",
        "# Create the evaluators for cross-validation\n",
        "evaluator1 = BinaryClassificationEvaluator(labelCol=label)\n",
        "evaluator2 = MulticlassClassificationEvaluator(labelCol=label)\n",
        "\n",
        "# Create the cross-validator\n",
        "cv = CrossValidator(estimator=pipeline,\n",
        "                    estimatorParamMaps=paramGrid,\n",
        "                    evaluator=evaluator1,\n",
        "                    numFolds=5)\n",
        "\n",
        "# Fit the model to the training data\n",
        "cvModel = cv.fit(train_resampled)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = cvModel.transform(test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = evaluator2.evaluate(predictions, {evaluator2.metricName: \"accuracy\"})\n",
        "f1 = evaluator2.evaluate(predictions, {evaluator2.metricName: \"f1\"})\n",
        "recall = evaluator2.evaluate(predictions, {evaluator2.metricName: \"weightedRecall\"})\n",
        "roc_auc = evaluator1.evaluate(predictions)\n",
        "confusion_matrix = predictions.groupBy(label).pivot(\"prediction\").count().na.fill(0).orderBy(label).collect()\n",
        "\n",
        "# Select the probability and label columns\n",
        "preds_and_labels = predictions.select(['probability', label])\n",
        "\n",
        "# Extract the probability of the positive class\n",
        "preds_and_labels = preds_and_labels.withColumn('prob_of_positive', col('probability')[1])\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "preds_and_labels_pd = preds_and_labels.toPandas()\n",
        "\n",
        "# Calculate the KS statistic\n",
        "ks_stat = preds_and_labels_pd.groupby(label)['prob_of_positive'].apply(lambda x: x.mean())\n",
        "ks_value = ks_stat.diff().abs().max()\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1:\", f1)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"ROC-AUC:\", roc_auc)\n",
        "print(\"Confusion Matrix:\", confusion_matrix)\n",
        "print(\"KS:\", ks_value)"
      ],
      "metadata": {
        "id": "sGJtUQ2-rzfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Himport pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def calc_iv(df, target):\n",
        "    \"\"\"\n",
        "    Calcula el Information Value (IV) de todas las variables en un DataFrame.\n",
        "\n",
        "    Parámetros:\n",
        "    df: DataFrame de pandas que contiene los datos.\n",
        "    target: Nombre de la columna que contiene la variable objetivo.\n",
        "\n",
        "    Retorna:\n",
        "    iv_df: DataFrame que contiene el IV para cada variable.\n",
        "    \"\"\"\n",
        "    # Crear un DataFrame vacío para almacenar los resultados\n",
        "    iv_df = pd.DataFrame(columns=['Variable', 'IV'])\n",
        "\n",
        "    # Iterar sobre todas las columnas en el DataFrame\n",
        "    for col in df.columns:\n",
        "        if col != target:\n",
        "            # Calcular la frecuencia de cada valor en la variable\n",
        "            freq_df = df.groupby(col)[target].agg(['count', 'sum'])\n",
        "\n",
        "            # Calcular el porcentaje de eventos y no eventos en cada grupo\n",
        "            freq_df['event_rate'] = freq_df['sum'] / freq_df['sum'].sum()\n",
        "            freq_df['non_event_rate'] = (freq_df['count'] - freq_df['sum']) / (freq_df['count'].sum() - freq_df['sum'].sum())\n",
        "\n",
        "            # Calcular el WOE para cada grupo\n",
        "            freq_df['woe'] = np.log(freq_df['non_event_rate'] / freq_df['event_rate'])\n",
        "\n",
        "            # Calcular el IV para la variable\n",
        "            iv = ((freq_df['non_event_rate'] - freq_df['event_rate']) * freq_df['woe']).sum()\n",
        "\n",
        "            # Agregar el resultado al DataFrame de resultados\n",
        "            iv_df = iv_df.append({'Variable': col, 'IV': iv}, ignore_index=True)\n",
        "\n",
        "    return iv_df\n",
        "\n",
        "# Especificar la columna de etiquetas\n",
        "label_col = \"label_column\"\n",
        "\n",
        "# Calcular el IV para todas las variables\n",
        "iv_df = calc_iv(df, label_col)\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(iv_df)"
      ],
      "metadata": {
        "id": "S-oHb-8W_n08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "train, test = df_2.randomSplit([0.7, 0.3], seed=12345)\n",
        "\n",
        "# Undersampling para tratar el desbalanceo de clases\n",
        "ratio = train.where(col('Malo_dias_tot') == 1).count() / train.where(col('Malo_dias_tot') == 0).count()\n",
        "train = train.sampleBy('Malo_dias_tot', fractions={0: ratio, 1: 1.0}, seed=12345)\n",
        "\n",
        "# Definir las etapas del pipeline\n",
        "stages = []\n",
        "categoricalColumns = [c for c in train.columns if c != 'Malo_dias_tot' and train.schema[c].dataType == StringType()]\n",
        "for categoricalCol in categoricalColumns:\n",
        "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
        "    stages += [stringIndexer]\n",
        "label_stringIdx = StringIndexer(inputCol=\"Malo_dias_tot\", outputCol=\"label\")\n",
        "stages += [label_stringIdx]\n",
        "numericCols = [c for c in train.columns if c != 'Malo_dias_tot' and train.schema[c].dataType != StringType()]\n",
        "assemblerInputs = [c + \"Index\" for c in categoricalColumns] + numericCols\n",
        "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
        "stages += [assembler]\n",
        "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
        "stages += [rf]\n",
        "pipeline = Pipeline(stages=stages)\n",
        "\n",
        "# Definir la cuadrícula de parámetros y el evaluador para la validación cruzada\n",
        "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\")\n",
        "\n",
        "# Definir el objeto de validación cruzada\n",
        "crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=3)\n",
        "\n",
        "# Ajustar el modelo con validación cruzada\n",
        "cvModel = crossval.fit(train)\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "predictions = cvModel.transform(test)\n",
        "\n",
        "# Calcular métricas de evaluación\n",
        "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
        "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
        "print(\"KS:\", metrics.areaUnderROC)\n",
        "metrics = MulticlassMetrics(predictionAndLabels)\n",
        "print(\"Confusion matrix:\", metrics.confusionMatrix().toArray())\n",
        "print(\"Accuracy:\", metrics.accuracy)\n",
        "print(\"Recall:\", metrics.recall(1))\n",
        "print(\"F1:\", metrics.fMeasure(1.0))\n",
        "evaluator.setMetricName(\"areaUnderROC\")\n",
        "print(\"ROC-AUC:\", evaluator.evaluate(predictions))\n",
        "\n",
        "# Mostrar las variables más importantes ordenadas por importancia\n",
        "featureImportances = cvModel.bestModel.stages[-1].featureImportances.toArray()\n",
        "for idx in featureImportances.argsort()[::-1]:\n",
        "    print(assemblerInputs[idx], featureImportances[idx])\n"
      ],
      "metadata": {
        "id": "r8IfiW3v0Bae"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}