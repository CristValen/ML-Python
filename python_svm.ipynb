{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIMhIxFPAP4vCuWjkIZAH2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristValen/ML-Python/blob/main/python_svm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vhTfpo5mmUZ"
      },
      "outputs": [],
      "source": [
        "#svm\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Establecer la semilla para el generador de números aleatorios\n",
        "random_state = 42\n",
        "\n",
        "# Convertir el DataFrame de PySpark a Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Dividir el conjunto de datos en train y test\n",
        "X = pandas_df.drop('Malos_Dias_tot', axis=1)\n",
        "y = pandas_df['Malos_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Estandarizar las variables\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Entrenar el modelo SVM con validación cruzada\n",
        "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
        "svc = svm.SVC(probability=True, random_state=random_state)\n",
        "clf = GridSearchCV(svc, parameters)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predecir valores para el conjunto de test\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "# Calcular métricas\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calcular el estadístico KS\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test_scaled)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Obtener las 10 variables más importantes del modelo\n",
        "feature_importances = pd.DataFrame(clf.best_estimator_.coef_[0], index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Establecer la semilla para el generador de números aleatorios\n",
        "random_state = 42\n",
        "\n",
        "# Convertir el DataFrame de PySpark a Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Dividir el conjunto de datos en train y test\n",
        "X = pandas_df.drop('Malos_Dias_tot', axis=1)\n",
        "y = pandas_df['Malos_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Estandarizar las variables\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Aplicar SMOTE al conjunto de entrenamiento\n",
        "sm = SMOTE(random_state=random_state)\n",
        "X_train_resampled, y_train_resampled = sm.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Entrenar el modelo SVM\n",
        "clf = svm.SVC(kernel='linear', probability=True, random_state=random_state)\n",
        "clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predecir valores para el conjunto de test\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "# Calcular métricas\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calcular el estadístico KS\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test_scaled)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Obtener las 10 variables más importantes del modelo\n",
        "feature_importances = pd.DataFrame(clf.coef_[0], index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n",
        "\n",
        "#\n"
      ],
      "metadata": {
        "id": "p72DoTO3mpeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#undersampling\n",
        "# Establecer la semilla para el generador de números aleatorios\n",
        "random_state = 42\n",
        "\n",
        "# Convertir el DataFrame de PySpark a Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Dividir el conjunto de datos en train y test\n",
        "X = pandas_df.drop('Malos_Dias_tot', axis=1)\n",
        "y = pandas_df['Malos_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Aplicar RandomUnderSampler al conjunto de entrenamiento\n",
        "rus = RandomUnderSampler(random_state=random_state)\n",
        "X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "# Crear una instancia de StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Ajustar el escalador a los datos de entrenamiento\n",
        "scaler.fit(X_train_resampled)\n",
        "\n",
        "# Transformar los datos de entrenamiento y prueba\n",
        "X_train_resampled = scaler.transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Entrenar el modelo SVM con validación cruzada\n",
        "param_grid = {'C': [0.1, 1, 10], 'gamma': [1, 0.1, 0.01]}\n",
        "grid = GridSearchCV(svm.SVC(kernel='linear', probability=True, random_state=random_state), param_grid, refit=True)\n",
        "grid.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predecir valores para el conjunto de test\n",
        "y_pred = grid.predict(X_test)\n",
        "\n",
        "# Calcular métricas\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calcular el estadístico KS\n",
        "fpr, tpr, thresholds = roc_curve(y_test, grid.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Obtener las 10 variables más importantes del modelo\n",
        "feature_importances = pd.DataFrame(grid.best_estimator_.coef_[0], index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sQmvjPzmnIJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Establecer la semilla para el generador de números aleatorios\n",
        "random_state = 42\n",
        "\n",
        "# Convertir el DataFrame de PySpark a Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Dividir el conjunto de datos en train y test\n",
        "X = pandas_df.drop('Malos_Dias_tot', axis=1)\n",
        "y = pandas_df['Malos_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Aplicar ADASYN al conjunto de entrenamiento\n",
        "adasyn = ADASYN(random_state=random_state)\n",
        "X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)\n",
        "\n",
        "# Crear una instancia de StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Ajustar el escalador a los datos de entrenamiento\n",
        "scaler.fit(X_train_resampled)\n",
        "\n",
        "# Transformar los datos de entrenamiento y prueba\n",
        "X_train_resampled = scaler.transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Entrenar el modelo SVM con validación cruzada\n",
        "param_grid = {'C': [0.1, 1, 10], 'gamma': [1, 0.1, 0.01]}\n",
        "grid = GridSearchCV(svm.SVC(kernel='linear', probability=True, random_state=random_state), param_grid, refit=True)\n",
        "grid.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predecir valores para el conjunto de test\n",
        "y_pred = grid.predict(X_test)\n",
        "\n",
        "# Calcular métricas\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calcular el estadístico KS\n",
        "fpr, tpr, thresholds = roc_curve(y_test, grid.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Obtener las 10 variables más importantes del modelo\n",
        "feature_importances = pd.DataFrame(grid.best_estimator_.coef_[0], index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jVm7_ta7nYyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Establecer la semilla para el generador de números aleatorios\n",
        "random_state = 42\n",
        "\n",
        "# Convertir el DataFrame de PySpark a Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Dividir el conjunto de datos en train y test\n",
        "X = pandas_df.drop('Malos_Dias_tot', axis=1)\n",
        "y = pandas_df['Malos_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Aplicar TomekLinks al conjunto de entrenamiento\n",
        "tl = TomekLinks()\n",
        "X_train_resampled, y_train_resampled = tl.fit_resample(X_train, y_train)\n",
        "\n",
        "# Crear una instancia de StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Ajustar el escalador a los datos de entrenamiento\n",
        "scaler.fit(X_train_resampled)\n",
        "\n",
        "# Transformar los datos de entrenamiento y prueba\n",
        "X_train_resampled = scaler.transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Definir los parámetros para la búsqueda en grilla\n",
        "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
        "\n",
        "# Crear el objeto GridSearchCV\n",
        "grid_search = GridSearchCV(svm.SVC(probability=True, random_state=random_state), param_grid, cv=5)\n",
        "\n",
        "# Entrenar el modelo SVM con búsqueda en grilla\n",
        "grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predecir valores para el conjunto de test\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "# Calcular métricas\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calcular el estadístico KS\n",
        "fpr, tpr, thresholds = roc_curve(y_test, grid_search.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Obtener las 10 variables más importantes del modelo\n",
        "feature_importances = pd.DataFrame(grid_search.best_estimator_.coef_[0], index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n",
        "\n"
      ],
      "metadata": {
        "id": "3MlMop7Rnn-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sin cross validation\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 42\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X = pandas_df.drop('Malos_Dias_tot', axis=1)\n",
        "y = pandas_df['Malos_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Standardize the variables\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the SVM model without cross-validation\n",
        "svc = svm.SVC(probability=True, random_state=random_state)\n",
        "svc.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict values for the test set\n",
        "y_pred = svc.predict(X_test_scaled)\n",
        "\n",
        "# Calculate metrics\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate the KS statistic\n",
        "fpr, tpr, thresholds = roc_curve(y_test, svc.predict_proba(X_test_scaled)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "feature_importances = pd.DataFrame(svc.coef_[0], index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kLC_hmQw6_E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 42\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X = pandas_df.drop('Malos_Dias_tot', axis=1)\n",
        "y = pandas_df['Malos_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Standardize the variables\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Apply SMOTE to the training set\n",
        "sm = SMOTE(random_state=random_state)\n",
        "X_train_resampled, y_train_resampled = sm.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Train the SVM model without cross-validation\n",
        "clf = svm.SVC(kernel='linear', probability=True, random_state=random_state)\n",
        "clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict values for the test set\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "# Calculate metrics\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate the KS statistic\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test_scaled)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "feature_importances = pd.DataFrame(clf.coef_[0], index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n"
      ],
      "metadata": {
        "id": "fK3m-f5D73Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 42\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X = pandas_df.drop('Malos_Dias_tot', axis=1)\n",
        "y = pandas_df['Malos_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Apply RandomUnderSampler to the training set\n",
        "rus = RandomUnderSampler(random_state=random_state)\n",
        "X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data\n",
        "scaler.fit(X_train_resampled)\n",
        "\n",
        "# Transform the training and test data\n",
        "X_train_resampled = scaler.transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the SVM model without cross-validation\n",
        "clf = svm.SVC(kernel='linear', probability=True, random_state=random_state)\n",
        "clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict values for the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate the KS statistic\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "feature_importances = pd.DataFrame(clf.coef_[0], index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n"
      ],
      "metadata": {
        "id": "GzRu8wmc8ZAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import ADASYN\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 42\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X = pandas_df.drop('Malos_Dias_tot', axis=1)\n",
        "y = pandas_df['Malos_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Apply ADASYN to the training set\n",
        "adasyn = ADASYN(random_state=random_state)\n",
        "X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data\n",
        "scaler.fit(X_train_resampled)\n",
        "\n",
        "# Transform the training and test data\n",
        "X_train_resampled = scaler.transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the SVM model without cross-validation\n",
        "clf = svm.SVC(kernel='linear', probability=True, random_state=random_state)\n",
        "clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict values for the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate the KS statistic\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "feature_importances = pd.DataFrame(clf.coef_[0], index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n"
      ],
      "metadata": {
        "id": "55hF-yWA8ZH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 42\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X = pandas_df.drop('Malos_Dias_tot', axis=1)\n",
        "y = pandas_df['Malos_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Apply TomekLinks to the training set\n",
        "tl = TomekLinks()\n",
        "X_train_resampled, y_train_resampled = tl.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data\n",
        "scaler.fit(X_train_resampled)\n",
        "\n",
        "# Transform the training and test data\n",
        "X_train_resampled = scaler.transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the SVM model without cross-validation\n",
        "clf = svm.SVC(kernel='linear', probability=True, random_state=random_state)\n",
        "clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict values for the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate the KS statistic\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "feature_importances = pd.DataFrame(clf.coef_[0], index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n"
      ],
      "metadata": {
        "id": "je6qKRmA8ZKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gasHXNhX8ZN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UghiLkLQ8ZSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#random forest\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 0\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X = pandas_df.drop('Malo_Dias_tot', axis=1)\n",
        "y = pandas_df['Malo_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Transform the training and test data\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the Random Forest model without cross-validation\n",
        "clf = RandomForestClassifier(random_state=random_state)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict values for the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate the KS statistic\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "feature_importances = pd.DataFrame(clf.feature_importances_, index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n",
        "\n"
      ],
      "metadata": {
        "id": "hr0MIUTB6rDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 0\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X = pandas_df.drop('Malo_Dias_tot', axis=1)\n",
        "y = pandas_df['Malo_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Apply SMOTE to the training set\n",
        "sm = SMOTE(random_state=random_state)\n",
        "X_train_resampled, y_train_resampled = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data\n",
        "scaler.fit(X_train_resampled)\n",
        "\n",
        "# Transform the training and test data\n",
        "X_train_resampled = scaler.transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the Random Forest model without cross-validation\n",
        "clf = RandomForestClassifier(random_state=random_state)\n",
        "clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict values for the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate the KS statistic\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "feature_importances = pd.DataFrame(clf.feature_importances_, index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n"
      ],
      "metadata": {
        "id": "iorfuzKbDm8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 0\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X = pandas_df.drop('Malo_Dias_tot', axis=1)\n",
        "y = pandas_df['Malo_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Apply RandomUnderSampler to the training set\n",
        "rus = RandomUnderSampler(random_state=random_state)\n",
        "X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data\n",
        "scaler.fit(X_train_resampled)\n",
        "\n",
        "# Transform the training and test data\n",
        "X_train_resampled = scaler.transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the Random Forest model without cross-validation\n",
        "clf = RandomForestClassifier(random_state=random_state)\n",
        "clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict values for the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate the KS statistic\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "feature_importances = pd.DataFrame(clf.feature_importances_, index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n"
      ],
      "metadata": {
        "id": "pmx9xDOQDnPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import ADASYN\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 0\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X = pandas_df.drop('Malo_Dias_tot', axis=1)\n",
        "y = pandas_df['Malo_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Apply ADASYN to the training set\n",
        "adasyn = ADASYN(random_state=random_state)\n",
        "X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data\n",
        "scaler.fit(X_train_resampled)\n",
        "\n",
        "# Transform the training and test data\n",
        "X_train_resampled = scaler.transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the Random Forest model without cross-validation\n",
        "clf = RandomForestClassifier(random_state=random_state)\n",
        "clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict values for the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate the KS statistic\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "feature_importances = pd.DataFrame(clf.feature_importances_, index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n"
      ],
      "metadata": {
        "id": "vlz_5d1wDnVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### decision tree\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 0\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X = pandas_df.drop('Malo_Dias_tot', axis=1)\n",
        "y = pandas_df['Malo_Dias_tot']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Transform the training and test data\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the Decision Tree model without cross-validation\n",
        "clf = DecisionTreeClassifier(random_state=random_state)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict values for the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate the KS statistic\n",
        "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
        "ks = np.max(tpr - fpr)\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "feature_importances = pd.DataFrame(clf.feature_importances_, index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "print(feature_importances.head(10))\n"
      ],
      "metadata": {
        "id": "FNjA5XJKDnYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###pyspark svm\n",
        "\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "random_state = 0\n",
        "\n",
        "# Convert the PySpark DataFrame to Pandas\n",
        "pandas_df = df_2.toPandas()\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train, test = df_2.randomSplit([0.8, 0.2], seed=random_state)\n",
        "\n",
        "# Define the feature columns\n",
        "feature_cols = [col for col in train.columns if col != 'Malo_Dias_tot']\n",
        "\n",
        "# Create a VectorAssembler to combine the feature columns into a single vector column\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "\n",
        "# Create a StandardScaler to standardize the features\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=True)\n",
        "\n",
        "# Train the Support Vector Machine model without cross-validation\n",
        "svm = LinearSVC(featuresCol='scaledFeatures', labelCol='Malo_Dias_tot', maxIter=10, regParam=0.1)\n",
        "\n",
        "# Create a pipeline to chain the assembler, scaler and SVM together\n",
        "pipeline = Pipeline(stages=[assembler, scaler, svm])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model = pipeline.fit(train)\n",
        "\n",
        "# Predict values for the test set\n",
        "predictions = model.transform(test)\n",
        "\n",
        "# Calculate metrics\n",
        "tp = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 1)].count()\n",
        "tn = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 0)].count()\n",
        "fp = predictions[(predictions.Malo_Dias_tot == 0) & (predictions.prediction == 1)].count()\n",
        "fn = predictions[(predictions.Malo_Dias_tot == 1) & (predictions.prediction == 0)].count()\n",
        "\n",
        "print(f'Confusion Matrix:\\n[[{tn} {fp}]\\n [{fn} {tp}]]')\n",
        "\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1_score = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1_score}')\n",
        "\n",
        "# Calculate the KS statistic\n",
        "pdf = predictions.select('Malo_Dias_tot', 'rawPrediction').toPandas()\n",
        "fpr, tpr, thresholds = roc_curve(pdf['Malo_Dias_tot'], pdf['rawPrediction'].apply(lambda x: x[1]))\n",
        "ks = max(tpr - fpr)\n",
        "\n",
        "print(f'KS: {ks}')\n",
        "\n",
        "# Get the top 10 most important variables of the model\n",
        "importances = model.stages[-1].coefficients.toArray()\n",
        "importance_df = pd.DataFrame(list(zip(feature_cols, importances)), columns=['Feature', 'Importance']).sort_values('Importance', ascending=False)\n",
        "print(importance_df.head(10))\n",
        "\n"
      ],
      "metadata": {
        "id": "KPaUVr82JJez"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}